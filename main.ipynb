{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "from itertools import product\n",
    "from tabulate import tabulate\n",
    "from joblib import Parallel, delayed\n",
    "from multiprocessing import Lock\n",
    "from sklearn.metrics import median_absolute_error, max_error, mean_squared_error, mean_absolute_error, r2_score, explained_variance_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from Utils import Utils\n",
    "\n",
    "from DecisionTreeRegressor import DecisionTreeRegressor\n",
    "from RandomForestRegressor import RandomForestRegressor\n",
    "from LinearRegression import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading and splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./NSO_Population_Sex_dataset/NSO_POPULATION_DATA_CLEANED.csv')\n",
    "\n",
    "feature_cols = ['District', 'Sex', 'Year', 'Population_Growth_Rate','Average_Population']\n",
    "X = pd.get_dummies(df[feature_cols], columns=['District', 'Sex'])\n",
    "y = df[\"Population\"]\n",
    "\n",
    "Custom_Implementations_Number_Of_Runs = 5\n",
    "NUM_CORES = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Implementations - Decision Tree, Random Forest, Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Trees Hyperparameter tuning\n",
    "Custom_Decision_Tree_Best_MSE_Over_Runs = float('inf')\n",
    "Custom_Decision_Tree_Best_Parameters_Over_Runs = None\n",
    "\n",
    "Custom_Decision_Tree_Parameters = {\n",
    "    'min_samples_split': [2, 3, 4, 5, 10, 15, 20],\n",
    "    'max_depth': [10, 25, 50, 75, 100, 125, 150, 175, 200]\n",
    "}\n",
    "\n",
    "for run in range(Custom_Implementations_Number_Of_Runs):\n",
    "    # Reshuffling data before each run\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    \n",
    "    # Initialise variables for each run\n",
    "    Custom_Decision_Tree_Best_MSE = float('inf')\n",
    "    Custom_Decision_Tree_Best_Parameters = None\n",
    "\n",
    "    # Iterate over all combinations of Decision Tree hyperparameters\n",
    "    for min_samples_split in Custom_Decision_Tree_Parameters['min_samples_split']:\n",
    "        for max_depth in Custom_Decision_Tree_Parameters['max_depth']:\n",
    "            # Instantiate and train your custom Decision Tree Regressor\n",
    "            Custom_Decision_Tree_Regressor_Temp = DecisionTreeRegressor(min_samples_split=min_samples_split, max_depth=max_depth)\n",
    "            Custom_Decision_Tree_Regressor_Temp.fit(X_train.values, y_train.values)\n",
    "\n",
    "            # Make predictions\n",
    "            Custom_Decision_Tree_Regressor_Temp_Predictions = Custom_Decision_Tree_Regressor_Temp.predict(X_test.values)\n",
    "\n",
    "            # Calculate Mean Squared Error\n",
    "            Custom_Decision_Tree_Regressor_Temp_MSE = mean_squared_error(y_test, Custom_Decision_Tree_Regressor_Temp_Predictions)\n",
    "\n",
    "            # Check if current combination of hyperparameters gives a better result\n",
    "            if Custom_Decision_Tree_Regressor_Temp_MSE < Custom_Decision_Tree_Best_MSE:\n",
    "                Custom_Decision_Tree_Best_MSE = Custom_Decision_Tree_Regressor_Temp_MSE\n",
    "                Custom_Decision_Tree_Best_Parameters = {'min_samples_split': min_samples_split, 'max_depth': max_depth}\n",
    "\n",
    "    print(f\"Run {run + 1}: Best Parameters - {Custom_Decision_Tree_Best_Parameters}, Best MSE - {Custom_Decision_Tree_Best_MSE}\")\n",
    "\n",
    "    if Custom_Decision_Tree_Best_MSE < Custom_Decision_Tree_Best_MSE_Over_Runs:\n",
    "        Custom_Decision_Tree_Best_MSE_Over_Runs = Custom_Decision_Tree_Best_MSE\n",
    "        Custom_Decision_Tree_Best_Parameters_Over_Runs = Custom_Decision_Tree_Best_Parameters\n",
    "\n",
    "print(f\"\\nOverall Best Parameters - {Custom_Decision_Tree_Best_Parameters_Over_Runs}, Best MSE - {Custom_Decision_Tree_Best_MSE_Over_Runs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Hyperparameter Tuning\n",
    "Custom_Random_Forest_Best_MSE_Over_Runs = float('inf')\n",
    "Custom_Random_Forest_Best_Parameters_Over_Runs = None\n",
    "\n",
    "Custom_Random_Forest_Parameters = {\n",
    "    'n_estimators': [75, 100, 125],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'max_depth': [25, 50, 75, 100]\n",
    "}\n",
    "\n",
    "for run in range(Custom_Implementations_Number_Of_Runs):\n",
    "    # Reshuffling data before each run\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "    # Initialise variables for each run\n",
    "    Custom_Random_Forest_Best_MSE = float('inf')\n",
    "    Custom_Random_Forest_Best_Parameters = None\n",
    "\n",
    "    # Iterate over all combinations of Random Forest hyperparameters\n",
    "    for n_estimators in Custom_Random_Forest_Parameters['n_estimators']:\n",
    "        for min_sample_split in Custom_Random_Forest_Parameters['min_samples_split']:\n",
    "            for max_depth in Custom_Random_Forest_Parameters['max_depth']:\n",
    "                # Instantiate and train your custom Random Forest Regressor\n",
    "                Custom_Random_Forest_Regressor_Temp = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, min_samples_split=min_sample_split)\n",
    "                Custom_Random_Forest_Regressor_Temp.fit(X_train, y_train)\n",
    "\n",
    "                # Make predictions\n",
    "                Custom_Random_Forest_Regressor_Temp_Predictions = Custom_Random_Forest_Regressor_Temp.predict(X_test)\n",
    "\n",
    "                # Calculate Mean Squared Error\n",
    "                Custom_Random_Forest_Regressor_Temp_MSE = mean_squared_error(y_test, Custom_Random_Forest_Regressor_Temp_Predictions)\n",
    "\n",
    "                # Check if the current combination of hyperparameters gives a better result\n",
    "                if Custom_Random_Forest_Regressor_Temp_MSE < Custom_Random_Forest_Best_MSE:\n",
    "                    Custom_Random_Forest_Best_MSE = Custom_Random_Forest_Regressor_Temp_MSE\n",
    "                    Custom_Random_Forest_Best_Parameters = {'n_estimators': n_estimators, 'min_samples_split': min_sample_split, 'max_depth': max_depth }\n",
    "\n",
    "    print(f\"Run {run + 1}: Best Parameters - {Custom_Random_Forest_Best_Parameters}, Best MSE - {Custom_Random_Forest_Best_MSE}\")\n",
    "\n",
    "    if Custom_Random_Forest_Best_MSE < Custom_Random_Forest_Best_MSE_Over_Runs:\n",
    "        Custom_Random_Forest_Best_MSE_Over_Runs = Custom_Random_Forest_Best_MSE\n",
    "        Custom_Random_Forest_Best_Parameters_Over_Runs = Custom_Random_Forest_Best_Parameters\n",
    "\n",
    "print(f\"\\nOverall Best Parameters - {Custom_Random_Forest_Best_Parameters_Over_Runs}, Best MSE - {Custom_Random_Forest_Best_MSE_Over_Runs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Hyperparameter Tuning\n",
    "Custom_Random_Forest_Parameters = {\n",
    "    'n_estimators': [50, 75, 100, 125],\n",
    "    'min_samples_split': [2, 5, 10, 15],\n",
    "    'max_depth': [25, 50, 75, 100]\n",
    "}\n",
    "\n",
    "# Create all combinations of hyperparameters\n",
    "Custom_Random_Forest_Hyperparameter_Combinations = list(product(\n",
    "    Custom_Random_Forest_Parameters['n_estimators'],\n",
    "    Custom_Random_Forest_Parameters['min_samples_split'],\n",
    "    Custom_Random_Forest_Parameters['max_depth']\n",
    "))\n",
    "\n",
    "# Function to train and evaluate the model for a specific parameter combination\n",
    "def train_evaluate_model(params, X_train, y_train, X_test, y_test):\n",
    "    n_estimators, min_sample_split, max_depth = params\n",
    "    model = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, min_samples_split=min_sample_split)\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    return mse, params\n",
    "\n",
    "# Initialize the dictionary with default values\n",
    "Custom_Random_Forest_Avg_MSE_Over_Parameters = {params: 0.0 for params in Custom_Random_Forest_Hyperparameter_Combinations}\n",
    "best_params_over_runs = {'params': None, 'mse': float('inf')}\n",
    "\n",
    "# Loop over runs\n",
    "for run in range(Custom_Implementations_Number_Of_Runs):    \n",
    "    # Reshuffling data before each run\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    \n",
    "    # Parallelize the hyperparameter search\n",
    "    results = Parallel(n_jobs=NUM_CORES)(delayed(train_evaluate_model)(params, X_train, y_train, X_test, y_test) \n",
    "                                          for params in Custom_Random_Forest_Hyperparameter_Combinations)\n",
    "\n",
    "    # Accumulate MSE for each parameter combination\n",
    "    for mse, params in results:\n",
    "        Custom_Random_Forest_Avg_MSE_Over_Parameters[params] += mse\n",
    "\n",
    "    # Find the best parameters for the current run\n",
    "    best_params = min(results, key=lambda x: x[0])\n",
    "    print(f\"Run {run + 1}: Best Parameters - {best_params[1]}, Best MSE - {best_params[0]}\")\n",
    "\n",
    "    # Update overall best parameters\n",
    "    if best_params[0] < best_params_over_runs['mse']:\n",
    "        best_params_over_runs['params'] = best_params[1]\n",
    "        best_params_over_runs['mse'] = best_params[0]\n",
    "\n",
    "# Calculate the average MSE for each parameter combination\n",
    "for params, total_mse in Custom_Random_Forest_Avg_MSE_Over_Parameters.items():\n",
    "    Custom_Random_Forest_Avg_MSE_Over_Parameters[params] = total_mse / Custom_Implementations_Number_Of_Runs\n",
    "\n",
    "# Find the best parameters based on the lowest average MSE\n",
    "Custom_Random_Forest_Best_Parameters_Over_Runs = min(Custom_Random_Forest_Avg_MSE_Over_Parameters, key=Custom_Random_Forest_Avg_MSE_Over_Parameters.get)\n",
    "print(\"\\nOverall Best Random Forest Regressor Parameters based on average MSE over runs:\", Custom_Random_Forest_Best_Parameters_Over_Runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression Hyperparameter tuning\n",
    "Custom_Linear_Regression_Best_MSE_Over_Runs = float('inf')\n",
    "Custom_Linear_Regression_Best_Parameters_Over_Runs = None\n",
    "\n",
    "Custom_Linear_Regression_Parameters = {\n",
    "    'learning_rate': [0.001, 0.01, 0.1, 1],\n",
    "    'num_iterations': [50, 100, 250, 500, 750, 1000, 1250, 1500]\n",
    "}\n",
    "\n",
    "for run in range(Custom_Implementations_Number_Of_Runs):\n",
    "    # Reshuffling data before each run\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "    # Initialise variables for each run\n",
    "    Custom_Linear_Regression_Best_MSE = float('inf')\n",
    "    Custom_Linear_Regression_Best_Parameters = None\n",
    "\n",
    "    # Iterate over all combinations of hyperparameters\n",
    "    for learning_rate in Custom_Linear_Regression_Parameters['learning_rate']:\n",
    "        for num_iterations in Custom_Linear_Regression_Parameters['num_iterations']:\n",
    "            # Instantiate and train your custom Linear Regression\n",
    "            Custom_Linear_Regression_Temp = LinearRegression(learning_rate=learning_rate, num_iterations=num_iterations)\n",
    "            Custom_Linear_Regression_Temp.fit(X_train.values, y_train.values)\n",
    "\n",
    "            # Make predictions\n",
    "            Custom_Linear_Regression_Temp_Predictions = Custom_Linear_Regression_Temp.predict(X_test.values)\n",
    "\n",
    "            # Calculate Mean Squared Error\n",
    "            Custom_Linear_Regression_Temp_MSE = mean_squared_error(y_test, Custom_Linear_Regression_Temp_Predictions)\n",
    "\n",
    "            # Check if the current combination of hyperparameters gives a better result\n",
    "            if Custom_Linear_Regression_Temp_MSE < Custom_Linear_Regression_Best_MSE:\n",
    "                Custom_Linear_Regression_Best_MSE = Custom_Linear_Regression_Temp_MSE\n",
    "                Custom_Linear_Regression_Best_Parameters = {'learning_rate': learning_rate, 'num_iterations': num_iterations}\n",
    "\n",
    "    print(f\"Run {run + 1}: Best Parameters - {Custom_Linear_Regression_Best_Parameters}, Best MSE - {Custom_Linear_Regression_Best_MSE}\")\n",
    "\n",
    "    if Custom_Linear_Regression_Best_MSE < Custom_Linear_Regression_Best_MSE_Over_Runs:\n",
    "        Custom_Linear_Regression_Best_MSE_Over_Runs = Custom_Linear_Regression_Best_MSE\n",
    "        Custom_Linear_Regression_Best_Parameters_Over_Runs = Custom_Linear_Regression_Best_Parameters\n",
    "\n",
    "print(f\"\\nOverall Best Parameters - {Custom_Linear_Regression_Best_Parameters_Over_Runs}, Best MSE - {Custom_Linear_Regression_Best_MSE_Over_Runs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshuffling data before run\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Small constant to avoid division by very small values\n",
    "MBD_Denominator = np.where(np.abs(y_test) > 1e-10, y_test, 1e-10) \n",
    "\n",
    "# Each algorithm has the parameters set according to the best found through their corresponding hyperparameter tuning\n",
    "# Decision Tree Regressor\n",
    "start_time = time.time()\n",
    "Custom_Decision_Tree_Regressor = DecisionTreeRegressor(min_samples_split=Custom_Decision_Tree_Best_Parameters_Over_Runs['min_samples_split'], max_depth=Custom_Decision_Tree_Best_Parameters_Over_Runs['max_depth'])\n",
    "Custom_Decision_Tree_Regressor.fit(X_train.values, y_train.values)\n",
    "Custom_Decision_Tree_Regressor_Prediction = Custom_Decision_Tree_Regressor.predict(X_test.values)\n",
    "end_time = time.time()\n",
    "Custom_Decision_Tree_Regressor_Time = end_time - start_time\n",
    "Custom_Decision_Tree_Regressor_Prediction_MAE = mean_absolute_error(y_test, Custom_Decision_Tree_Regressor_Prediction)  # Mean Absolute Error\n",
    "Custom_Decision_Tree_Regressor_Prediction_MSE = mean_squared_error(y_test, Custom_Decision_Tree_Regressor_Prediction)  # Mean Squared Error\n",
    "Custom_Decision_Tree_Regressor_Prediction_RMSE = Utils.root_mean_squared_error(y_test, Custom_Decision_Tree_Regressor_Prediction)  # Root Mean Squared Error\n",
    "Custom_Decision_Tree_Regressor_Prediction_R2 = r2_score(y_test, Custom_Decision_Tree_Regressor_Prediction)  # R-squared\n",
    "Custom_Decision_Tree_Regressor_MAPE = np.mean(np.abs((y_test - Custom_Decision_Tree_Regressor_Prediction) / y_test)) * 100  # Mean Absolute Percentage Error\n",
    "Custom_Decision_Tree_Regressor_SMAPE = 2 * np.mean(np.abs(y_test - Custom_Decision_Tree_Regressor_Prediction) / (np.abs(y_test) + np.abs(Custom_Decision_Tree_Regressor_Prediction))) * 100  # Symmetric Mean Absolute Percentage Error\n",
    "Custom_Decision_Tree_Regressor_Explained_Variance = explained_variance_score(y_test, Custom_Decision_Tree_Regressor_Prediction)  # Explained Variance Score\n",
    "Custom_Decision_Tree_Regressor_MedianAE = median_absolute_error(y_test, Custom_Decision_Tree_Regressor_Prediction)  # Median Absolute Error\n",
    "Custom_Decision_Tree_Regressor_MBD = np.mean((y_test - Custom_Decision_Tree_Regressor_Prediction) / MBD_Denominator) * 100  # Mean Bias Deviation\n",
    "Custom_Decision_Tree_Regressor_MaxError = max_error(y_test, Custom_Decision_Tree_Regressor_Prediction)  # Maximum Error\n",
    "\n",
    "# Random Forest Regressor\n",
    "start_time = time.time()\n",
    "Custom_Random_Forest_Regressor = RandomForestRegressor(n_estimators=Custom_Random_Forest_Best_Parameters_Over_Runs['n_estimators'], max_depth=Custom_Random_Forest_Best_Parameters_Over_Runs['max_depth'], min_samples_split=Custom_Random_Forest_Best_Parameters_Over_Runs['min_samples_split'])\n",
    "Custom_Random_Forest_Regressor.fit(X_train, y_train)\n",
    "Custom_Random_Forest_Regressor_Prediction = Custom_Random_Forest_Regressor.predict(X_test)\n",
    "end_time = time.time()\n",
    "Custom_Random_Forest_Regressor_Time = end_time - start_time\n",
    "Custom_Random_Forest_Regressor_MAE = mean_absolute_error(y_test, Custom_Random_Forest_Regressor_Prediction)  # Mean Absolute Error\n",
    "Custom_Random_Forest_Regressor_MSE = mean_squared_error(y_test, Custom_Random_Forest_Regressor_Prediction)  # Mean Squared Error\n",
    "Custom_Random_Forest_Regressor_RMSE = Utils.root_mean_squared_error(y_test, Custom_Random_Forest_Regressor_Prediction)  # Root Mean Squared Error\n",
    "Custom_Random_Forest_Regressor_R2 = r2_score(y_test, Custom_Random_Forest_Regressor_Prediction)  # R-squared\n",
    "Custom_Random_Forest_Regressor_MAPE = np.mean(np.abs((y_test - Custom_Random_Forest_Regressor_Prediction) / y_test)) * 100  # Mean Absolute Percentage Error\n",
    "Custom_Random_Forest_Regressor_SMAPE = 2 * np.mean(np.abs(y_test - Custom_Random_Forest_Regressor_Prediction) / (np.abs(y_test) + np.abs(Custom_Random_Forest_Regressor_Prediction))) * 100  # Symmetric Mean Absolute Percentage Error\n",
    "Custom_Random_Forest_Regressor_Explained_Variance = explained_variance_score(y_test, Custom_Random_Forest_Regressor_Prediction)  # Explained Variance Score\n",
    "Custom_Random_Forest_Regressor_MedianAE = median_absolute_error(y_test, Custom_Random_Forest_Regressor_Prediction)  # Median Absolute Error\n",
    "Custom_Random_Forest_Regressor_MBD = np.mean((y_test - Custom_Random_Forest_Regressor_Prediction) / MBD_Denominator) * 100  # Mean Bias Deviation\n",
    "Custom_Random_Forest_Regressor_MaxError = max_error(y_test, Custom_Random_Forest_Regressor_Prediction)  # Maximum Error\n",
    "\n",
    "# Linear Regression\n",
    "start_time = time.time()\n",
    "Custom_Linear_Regression = LinearRegression(learning_rate=Custom_Linear_Regression_Best_Parameters_Over_Runs['learning_rate'], num_iterations=Custom_Linear_Regression_Best_Parameters_Over_Runs['num_iterations'])\n",
    "Custom_Linear_Regression.fit(X_train.values, y_train.values)\n",
    "Custom_Linear_Regression_Prediction = Custom_Linear_Regression.predict(X_test)\n",
    "end_time = time.time()\n",
    "Custom_Linear_Regression_Time = end_time - start_time\n",
    "Custom_Linear_Regression_MAE = mean_absolute_error(y_test, Custom_Linear_Regression_Prediction)  # Mean Absolute Error\n",
    "Custom_Linear_Regression_MSE = mean_squared_error(y_test, Custom_Linear_Regression_Prediction)  # Mean Squared Error\n",
    "Custom_Linear_Regression_RMSE = Utils.root_mean_squared_error(y_test, Custom_Linear_Regression_Prediction)  # Root Mean Squared Error\n",
    "Custom_Linear_Regression_R2 = r2_score(y_test, Custom_Linear_Regression_Prediction)  # R-squared\n",
    "Custom_Linear_Regression_MAPE = np.mean(np.abs((y_test - Custom_Linear_Regression_Prediction) / y_test)) * 100  # Mean Absolute Percentage Error\n",
    "Custom_Linear_Regression_SMAPE = 2 * np.mean(np.abs(y_test - Custom_Linear_Regression_Prediction) / (np.abs(y_test) + np.abs(Custom_Linear_Regression_Prediction))) * 100  # Symmetric Mean Absolute Percentage Error\n",
    "Custom_Linear_Regression_MedianAE = median_absolute_error(y_test, Custom_Linear_Regression_Prediction)  # Median Absolute Error\n",
    "Custom_Linear_Regression_Explained_Variance = explained_variance_score(y_test, Custom_Linear_Regression_Prediction)  # Explained Variance Score\n",
    "Custom_Linear_Regression_MBD = np.mean((y_test - Custom_Linear_Regression_Prediction) / MBD_Denominator) * 100  # Mean Bias Deviation\n",
    "Custom_Linear_Regression_MaxError = max_error(y_test, Custom_Linear_Regression_Prediction)  # Maximum Error\n",
    "\n",
    "# Print time taken for each algorithm\n",
    "print(\"Time taken for Decision Tree Regressor: {:.4f} seconds\".format(Custom_Decision_Tree_Regressor_Time))\n",
    "print(\"Time taken for Random Forest Regressor: {:.4f} seconds\".format(Custom_Random_Forest_Regressor_Time))\n",
    "print(\"Time taken for Linear Regression: {:.4f} seconds\".format(Custom_Linear_Regression_Time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scatter Plot - Actual vs Predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Custom Decision Tree vs Custom Random Forest vs Custom Linear Regression\n",
    "y_test_numeric = np.asarray(y_test, dtype=np.float64)\n",
    "Custom_Decision_Tree_Regressor_Prediction_Numeric = np.asarray(Custom_Decision_Tree_Regressor_Prediction, dtype=np.float64)\n",
    "Custom_Random_Forest_Regressor_Prediction_Numeric = np.asarray(Custom_Random_Forest_Regressor_Prediction, dtype=np.float64)\n",
    "Custom_Linear_Regression_Prediction_Numeric = np.asarray(Custom_Linear_Regression_Prediction, dtype=np.float64)\n",
    "\n",
    "plt.scatter(y_test, Custom_Decision_Tree_Regressor_Prediction_Numeric, alpha=0.7, marker='o', label='Custom Decision Tree')\n",
    "plt.scatter(y_test, Custom_Random_Forest_Regressor_Prediction_Numeric, alpha=0.7, marker='s', label='Custom Random Forest')\n",
    "plt.scatter(y_test, Custom_Linear_Regression_Prediction_Numeric, alpha=0.7, marker='^', label='Custom Linear Regression')\n",
    "\n",
    "# Regression lines\n",
    "plt.plot(np.unique(y_test), np.poly1d(np.polyfit(y_test, Custom_Decision_Tree_Regressor_Prediction_Numeric, 1))(np.unique(y_test)), color='blue', linestyle='--')\n",
    "plt.plot(np.unique(y_test), np.poly1d(np.polyfit(y_test, Custom_Random_Forest_Regressor_Prediction_Numeric, 1))(np.unique(y_test)), color='orange', linestyle='--')\n",
    "plt.plot(np.unique(y_test), np.poly1d(np.polyfit(y_test, Custom_Linear_Regression_Prediction_Numeric, 1))(np.unique(y_test)), color='green', linestyle='--')\n",
    "\n",
    "# Reference line\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], linestyle='-', color='black')\n",
    "\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Actual vs Predicted Values Scatter Plot - Custom Implementations')\n",
    "\n",
    "plt.legend(['Custom Decision Tree', 'Custom Random Forest', 'Custom Linear Regression'], loc='lower center', bbox_to_anchor=(0.5,-0.5))\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Line Plot - Actual vs Predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Custom Decision Tree vs Custom Random Forest vs Custom Linear Regression\n",
    "plt.plot(y_test.values, label='Actual', color='black', linestyle='-')\n",
    "plt.plot(Custom_Decision_Tree_Regressor_Prediction, label='Decision Tree', alpha=0.8, linestyle='--')\n",
    "plt.plot(Custom_Random_Forest_Regressor_Prediction, label='Random Forest', alpha=0.8, linestyle='--')\n",
    "plt.plot(Custom_Linear_Regression_Prediction, label='Linear Regression', alpha=0.8, linestyle='--')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Target Variable (Values)')\n",
    "plt.title('Actual vs Predicted Line Plot - Custom Implementations')\n",
    "plt.legend(loc='lower center', bbox_to_anchor=(0.5,-0.5))\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residual Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_decision_tree_residuals = y_test - Custom_Decision_Tree_Regressor_Prediction\n",
    "custom_random_forest_residuals = y_test - Custom_Random_Forest_Regressor_Prediction\n",
    "custom_linear_regression_residuals = y_test - Custom_Linear_Regression_Prediction\n",
    "\n",
    "# Plot for Custom Decision Tree\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, custom_decision_tree_residuals, alpha=0.6)\n",
    "plt.axhline(y=0, color='r', linestyle='-', linewidth=2)\n",
    "plt.grid(True)\n",
    "plt.xlabel('Observation')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Custom Decision Tree Residuals')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot for Custom Random Forest\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, custom_random_forest_residuals, alpha=0.6)\n",
    "plt.axhline(y=0, color='r', linestyle='-', linewidth=2)\n",
    "plt.grid(True)\n",
    "plt.xlabel('Observation')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Custom Random Forest Residuals')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot for Custom Linear Regression\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, custom_linear_regression_residuals, alpha=0.6)\n",
    "plt.axhline(y=0, color='r', linestyle='-', linewidth=2)\n",
    "plt.grid(True)\n",
    "plt.xlabel('Observation')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Custom Linear Regression Residuals')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Residuals vs. Predicted Values for all models\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(Custom_Decision_Tree_Regressor_Prediction, custom_decision_tree_residuals, label='Decision Tree Residuals')\n",
    "plt.scatter(Custom_Random_Forest_Regressor_Prediction, custom_random_forest_residuals, label='Random Forest Residuals')\n",
    "plt.scatter(Custom_Linear_Regression_Prediction, custom_linear_regression_residuals, label='Linear Regression Residuals')\n",
    "plt.axhline(y=0, color='r', linestyle='--', label='Residuals Mean')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals vs. Predicted Values')\n",
    "plt.legend(loc='lower center', bbox_to_anchor=(0.5,-0.25))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean Absolute Error, Mean Squared Error, Root Mean Squared Error, R Squared, Cross-Validation, Explained Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_data = {\n",
    "    '': ['Custom Decision Tree Regressor', 'Custom Random Forest Regressor', 'Custom Linear Regression'],\n",
    "    'Mean Absolute Error (MAE)': [Custom_Decision_Tree_Regressor_Prediction_MAE, Custom_Random_Forest_Regressor_MAE, Custom_Linear_Regression_MAE],\n",
    "    'Median Absolute Error (MedianAE)': [Custom_Decision_Tree_Regressor_MedianAE, Custom_Random_Forest_Regressor_MedianAE, Custom_Linear_Regression_MedianAE],\n",
    "    'Mean Squared Error (MSE)': [Custom_Decision_Tree_Regressor_Prediction_MSE, Custom_Random_Forest_Regressor_MSE, Custom_Linear_Regression_MSE],\n",
    "    'Root Mean Squared Error (RMSE)': [Custom_Decision_Tree_Regressor_Prediction_RMSE, Custom_Random_Forest_Regressor_RMSE, Custom_Linear_Regression_RMSE],\n",
    "    'R Squared (R\\u00b2)': [Custom_Decision_Tree_Regressor_Prediction_R2, Custom_Random_Forest_Regressor_R2, Custom_Linear_Regression_R2],\n",
    "    'Explained Variance': [Custom_Decision_Tree_Regressor_Explained_Variance, Custom_Random_Forest_Regressor_Explained_Variance, Custom_Linear_Regression_Explained_Variance],\n",
    "    'Mean Bias Deviation (MBD)': [Custom_Decision_Tree_Regressor_MBD, Custom_Random_Forest_Regressor_MBD, Custom_Linear_Regression_MBD],\n",
    "    'Maximum Error': [Custom_Decision_Tree_Regressor_MaxError, Custom_Random_Forest_Regressor_MaxError, Custom_Linear_Regression_MaxError],\n",
    "    'Mean Absolute Percentage Error (MAPE)': [Custom_Decision_Tree_Regressor_MAPE, Custom_Random_Forest_Regressor_MAPE, Custom_Linear_Regression_MAPE],\n",
    "    'Symmetric Mean Absolute Percentage Error (SMAPE)': [Custom_Decision_Tree_Regressor_SMAPE, Custom_Random_Forest_Regressor_SMAPE, Custom_Linear_Regression_SMAPE]\n",
    "}\n",
    "\n",
    "print(tabulate(table_data, headers='keys', tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual vs Predicted Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_table_data = {\n",
    "    'Actual Values': y_test,\n",
    "    'Custom Decision Tree Regressor': Custom_Decision_Tree_Regressor_Prediction,\n",
    "    'Custom Random Forest Regressor': Custom_Random_Forest_Regressor_Prediction,\n",
    "    'Custom Linear Regression': Custom_Linear_Regression_Prediction\n",
    "}\n",
    "\n",
    "print(tabulate(results_table_data, headers='keys', tablefmt='fancy_grid'))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(y_test, label='Actual Values', color='black', linestyle='-')\n",
    "sns.kdeplot(Custom_Decision_Tree_Regressor_Prediction, label='Decision Tree Predictions', alpha=0.8, linestyle='--')\n",
    "sns.kdeplot(Custom_Random_Forest_Regressor_Prediction, label='Random Forest Predictions', alpha=0.8, linestyle='--')\n",
    "sns.kdeplot(Custom_Linear_Regression_Prediction, label='Linear Regression Predictions', alpha=0.8, linestyle='--')\n",
    "plt.xlabel('Values')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Distribution of Actual vs Predicted Values')\n",
    "plt.legend(loc='lower center', bbox_to_anchor=(0.5,-0.4))\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
